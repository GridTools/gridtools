include:
  - remote: "https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml"

stages:
  - baseimage
  - build
  - test

.build_baseimage:
  stage: baseimage
  # we create a tag that depends on the SHA value of ci/base.Dockerfile, this way
  # a new base image is only built when the SHA of this file changes
  # If there are more dependency files that should change the tag-name of the base container
  # image, they can be added too.
  # Since the base image name is runtime dependent, we need to carry the value of it to
  # the following jobs via a dotenv file.
  before_script:
  # include build arguments in hash since we use a parameterized Docker file
  - DOCKER_TAG=`echo "$(cat $DOCKERFILE) $DOCKER_BUILD_ARGS" | sha256sum | head -c 16`
  - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/public/$ARCH/base/gridtools-ci:$DOCKER_TAG
  - echo "BASE_IMAGE=$PERSIST_IMAGE_NAME" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  variables:
    DOCKERFILE: ci/base.Dockerfile
    # change to 'always' if you want to rebuild, even if target tag exists already (if-not-exists is the default, i.e. we could also skip the variable)
    CSCS_REBUILD_POLICY: if-not-exists
    DOCKER_BUILD_ARGS: '["CUDA_VERSION=$CUDA_VERSION", "UBUNTU_VERSION=$UBUNTU_VERSION"]'
# build_baseimage_x86_64:
#   extends: [.container-builder-cscs-zen2, .build_baseimage]
#   variables:
#     CUDA_VERSION: 12.6.2
#     UBUNTU_VERSION: 22.04  
build_baseimage_aarch64:
  extends: [.container-builder-cscs-gh200, .build_baseimage]
  variables:
    CUDA_VERSION: 12.6.2
    CUDA_ARCH: sm_90
    UBUNTU_VERSION: 24.04
    SLURM_TIMELIMIT: 10


.build_image:
  stage: build
  variables:
    # make sure we use a unique name here, otherwise we could create a race condition, when multiple pipelines
    # are running.
    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/public/$ARCH/gridtools/gridtools-ci:$CI_COMMIT_SHA
    DOCKERFILE: ci/build.Dockerfile
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "BUILD_TYPE=release"]'
# .build_image_x86_64:
#   extends: [.container-builder-cscs-zen2, .build_image]
build_image_aarch64:
  extends: [.container-builder-cscs-gh200, .build_image]
  variables:
    SLURM_TIMELIMIT: 10

.test_helper:
  stage: test
  image: $CSCS_REGISTRY_PATH/public/$ARCH/gridtools/gridtools-ci:$CI_COMMIT_SHA
  variables:
    GTRUN_WITH_SLURM: False # since we are already in a SLURM job
    SLURM_JOB_NUM_NODES: 1
    SLURM_TIMELIMIT: 10
    CSCS_CUDA_MPS: 0

test_aarch64:
  extends: [.container-runner-daint-gh200, .test_helper]
  script:
  - uv run /build/pyutils/driver.py -v test --build-examples
  variables:
    SLURM_NTASKS: 1

test_aarch64_mpi:
  extends: [.container-runner-daint-gh200, .test_helper]
  script:
  - export LD_LIBRARY_PATH=/usr/lib64:$LD_LIBRARY_PATH
  - export LD_PRELOAD=/usr/lib64/libmpi_gtl_cuda.so
  - nvidia-smi
  - echo "Running tests with $SLURM_NTASKS tasks"
  - cd /gridtools && ./a.out
  - cd /build && ctest -L mpi --verbose
  variables:
    NVIDIA_VISIBLE_DEVICES: all
    SLURM_NTASKS: 4
    SLURM_GPUS_PER_TASK: 1
    MPICH_GPU_SUPPORT_ENABLED: 1
    USE_MPI: "YES"
    SLURM_MPI_TYPE: cray_shasta
    CSCS_ADDITIONAL_MOUNTS: '["/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/libmpi.so:/usr/local/lib/libmpi.so.12.1.8", "/opt/cray/pe/lib64/libpmi.so.0:/usr/lib64/libpmi.so.0", "/opt/cray/pe/lib64/libpmi2.so.0:/usr/lib64/libpmi2.so.0",  "/opt/cray/pals/1.4/lib/libpals.so.0:/usr/lib64/libpals.so.0",  "/usr/lib64/libgfortran.so.5:/usr/lib64/libgfortran.so.5", "/opt/cray/pe/mpich/8.1.28/gtl/lib/libmpi_gtl_cuda.so:/usr/lib64/libmpi_gtl_cuda.so"]'
